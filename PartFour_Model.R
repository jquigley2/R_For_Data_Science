###############################################################
#PART 4: MODEL
###############################################################
#22: ******************************* Introduction *****************************
#The goal of a model is to provide a simple low-dimensional summary of a dataset. 
#Ideally, the model will capture true ???signals??? (i.e. patterns generated by the phenomenon of interest), 
#and ignore ???noise??? (i.e. random variation that you???re not interested in). 
#Here we only cover ???predictive??? models, which, as the name suggests, generate predictions. 
#There is another type of model that we???re not going to discuss: ???data discovery??? models. 
#These models don???t make predictions, but instead help you discover interesting relationships within your data. 
#(These two categories of models are sometimes called supervised and unsupervised).

#In Ch. 23, Model Basics, you???ll learn how models work mechanistically, focussing on the important family of linear models. You???ll learn general tools for gaining insight into what a predictive model tells you about your data, focussing on simple simulated datasets.
#In Ch. 24, Model Building, you???ll learn how to use models to pull out known patterns in real data. Once you have recognised an important pattern it???s useful to make it explicit in a model, because then you can more easily see the subtler signals that remain.
#In Ch. 25, Many Models, you???ll learn how to use many simple models to help understand complex datasets. This is a powerful technique, but to access it you???ll need to combine modelling and programming tools.


#23: ******************************* Model Basics *****************************
#23.1: ***************************** Introduction *****************************
#Goal of modeling: provide a low-dimensional summary of a dataset by partitioning data into patterns and residuals.
#2 Parts to a Model:
  #1. Define a family of models which express a precise but generic pattern in the dataset. Linear, quadratic, etc...
  #2. Generate a fitted model by finding the model from your selected family which best fits your data.

#23.1.1: *************************** Prerequisites *****************************
library(tidyverse)
library(modelr)
options(na.action = na.warn)


#23.2: ***************************** A Simple Model ***************************** 
#Using the simulated data set sim1 from the modelr package.
?sim1
str(sim1)

ggplot(sim1, aes(x,y)) +
  geom_point()
#The relationship appears to be linear: y = a_0 + a_1 * x

#Generate a few linear models:

#create a tibble
models <- tibble(
  a1 = runif(250, -20, 40), #column 1 has 250 oservations ranging from -10 to 40
  a2 = runif(250,-5,5) #column 2 has 250 observations ranging from -5 to 5
)

#use geom_abline(), which takes a slope and intercept as parameters:
?geom_abline
ggplot(sim1, aes(x,y)) + 
  geom_abline(aes(intercept=a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()

#To measure the goodness of fit, turn the model family into a function which takes the model parameters and data 
#as inputs, and give predicted values.
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)

#Now, predict the RMSE for the model:
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

measure_distance(c(7, 1.5), sim1)

#Use purrr to measure the distance for all models:
#First, a helper function b/c the distance function expects the models as a numeric vector of length 2:
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

#Calculate across all models
models <- models %>%
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

#Examine results:
models

#Overlay the 10 best models onto the data, coloring by -dist:
ggplot(sim1, aes(x,y)) +
  geom_point(size = 2, color = "grey30") +
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist),
    data = filter(models, rank(dist) <=10)
  )

#We can think of these models as observations. Here, a scatterplot of a1 vs a2, colored by -dist.
#10 best models are again highlighted:
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <=10), size=4, color = "red") +
  geom_point(aes(color = -dist))

#We can also do a grid search:
grid <- expand.grid(
  a1 = seq(-5, 20, length=25),
  a2 = seq(0, 4, length=25)
) %>%
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>%
  ggplot(aes(a1, a2)) + 
  geom_point(data = filter(grid, rank(dist) <=10), size = 4, color = "red") + 
  geom_point(aes(colour = -dist))

#Now overlay these 10 best models:
ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, color = "grey30") +
  geom_abline(
    aes(intercept=a1, slope=a2, color=-dist),
    data = filter(grid, rank(dist)<=10)
  )

#Rather than making the grid finer and finer, use the Newton_Raphson minimization search with optim():
best <- optim(c(0,0), measure_distance, data = sim1)
best$par
#optim() will work with any family of models.

#graph this result:
ggplot(sim1, aes(x,y)) +
  geom_point(size = 2, color="grey30") + 
  geom_abline(intercept = best$par[1], slope = best$par[2])

#Since this is a linear model, we can use lm() to find the best model:
sim1_mod <- lm(y ~ x, data=sim1)

#examine the results:
coef(sim1_mod)
#exact same results as optim()!

#23.2.1: ***************************** Exercises *****************************
#1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates 
#a squared term. Fit a linear model to the simulated data below, and visualise the results. 
sim1a <- tibble(
  x = rep(1:10, each=3),
  y = x * 1.5 +6 + rt(length(x), df=2)
)

ggplot(sim1a, aes(x,y)) +
  geom_point() + 
  geom_smooth(method="lm", se=FALSE)

#Rerun a few times to generate different simulated datasets. What do you notice about the model?
simt <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 +6 + rt(length(x), df=2),
    .id = i
  )
}
 
sims <- map_df(1:12, simt)

ggplot(sims, aes(x,y)) +
  geom_point() + 
  geom_smooth(method="lm", color="red") +
  facet_wrap(~.id, ncol = 4)


#2. One way to make linear models more robust is to use a different distance measure. 
#For example, instead of root-mean-squared distance, you could use mean-absolute distance:
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}

#We need to define a function to take a numeric vector of length 2 (intercept and slope) and return the predictions:
make_prediction <- function(mod, data) {
  mod[1] + mod[2]*data$x
}


#Use optim() to fit this model to the simulated data above and compare it to the linear model.
best <- optim(c(0,0), measure_distance, data= sim1a)
best$par

#3. One challenge with performing numerical optimisation is that it???s only guaranteed to find 
#one local optimum. What???s the problem with optimising a three parameter model like this?
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}


#23.3: ***************************** Visualizing Models *****************************
#23.3.1: *************************** Predictions *****************************
#To visualize predictions, start by generating an evenly spaced grid of values covering the range of data.
#To do this, use modelr::data_grid()
grid <- sim1 %>%
  data_grid(x)

grid

#Next, add predictions using modelr::add_predictions(), which takes a data frame and a model:
grid <-grid %>%
  add_predictions(sim1_mod)

grid

#now plot the predictions
#http://vita.had.co.nz/papers/model-vis.html

ggplot(sim1, aes(x)) + 
  geom_point(aes(y=y)) +
  geom_line(aes(y=pred), data = grid, color="red", size=1)

#23.3.2: *************************** Residuals *****************************
#Predictions tell us the pattern, residuals show us what the model has missed.

#We add residuals to the data with modelr::add_residuals().  Here, we use the original dataset 
#rather than the manufactured grid.  To compute residuals, we need the actual y values:
sim1 <- sim1 %>%
  add_residuals(sim1_mod)

#result:
sim1

#To understand what the residuals tell us about the model, we can look at a frequency polygon:
ggplot(sim1, aes(resid)) +
  geom_freqpoly(binwidth = 0.5)

#how far away are the predictions from the observed values?

#recreate a plot using the residuals rather than the original predictor:
ggplot(sim1, aes(x, resid)) +
  geom_ref_line(h=0) +
  geom_point()
#if the result looks like random noise, your model has likely done a good job!


#23.3.3: *************************** Exercises *****************************
#1. Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. 
#Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using 
#loess() instead of lm(). How does the result compare to geom_smooth()?

#2. add_predictions() is paired with gather_predictions() and spread_predictions(). 
#How do these three functions differ?

#3. What does geom_ref_line() do? What package does it come from? 
#Why is displaying a reference line in plots showing residuals useful and important?

#4. Why might you want to look at a frequency polygon of absolute residuals? 
#What are the pros and cons compared to looking at the raw residuals?


#23.4: ***************************** Formulas and Model Families *****************************
#Formulas provide a general way of getting "special" behavior.  The capture the values of the variables 
#so they can be interpreted by the function.

#Most modelling functions in R use a standard conversion from formulas to functions.
#One simple conversion is y ~x translated to y = a_1 + a_2 * x.

#Use the model_matrix() function to see what R is actually doing.  It takes a df and a formula and 
#returns a tibble that defines the model equation: eaach column is associated with one coefficient in the model, 
# and the function is always y = a_1*out_1 + a_2*out_2.
df <- tribble(
  ~y, ~x1, ~x2,
  4,2,5,
  5,1,6
)
model_matrix(df, y~x1)

#R adds the intercept to the model via a column full of 1's, by default.
#If you don't want this column, explicitly drop it with -1:
model_matrix(df, y ~ x1 -1)

model_matrix(df, y ~x1 + x2)

#23.4.1: ***************************** Categorical variables *****************************
#When the predictor is categorical, like y ~ sex, sex obviously isn't a number.  So, R converts this to
#y = x_0 + x_1 * sex_male, where sex_male = 1 if male, 0 if female:

df <- tribble(
  ~sex, ~response,
  "male",1,
  "female", 2,
  "male",1
)

model_matrix(df, response ~ sex)

#If we focus on visualizing predictions, we needn't worry about parameterization.
#Let's look at the sim2 data set from modelr:

ggplot(sim2) +
  geom_point(aes(x,y))

#Fit to a model:
mod2 <- lm(y ~ x, data = sim2)

#generate predictions:
grid <- sim2 %>%
  data_grid(x) %>%
  add_predictions(mod2)

#review predictions
grid 

#overlay predictions on original data:
ggplot(sim2, aes(x)) + 
  geom_point(aes(y=y)) +
  geom_point(data = grid, aes(y=pred), color="red", size=4)


#23.4.2: ***************************** Interactions(continuous and categorical) *****************************
#sim3 contains a continuous and a categorical predictor.
#Visualize:
ggplot(sim3, aes(x1,y)) + 
  geom_point(aes(color=x2))


#Two possible models we could fit to this data:
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)

#When we add variables with +, the model estimates each effect independent of the others.
#When we multiply, we capture the interaction.  When we use *, both the interaction and the components
#are included in the model.  mod2 translates to: y = a_0 + (a_1*x_1) + (a_2 * x_2) + (a_12*x_1*x_2)

#To visualize the two models, we need 2 tricks:

  #1. We have 2 predictors, so need to give both to data_grid().  This will find all unique values
  #of x_1 and x_2 and then generate all combinations.
  
  #2. To generate predictions from each model simultaneously, use gather_predictions(), which adds each prediction
  #as a row.  The complement is spread_predictions(), which adds each prediction to a new column.

#Together:
grid <- sim3 %>%
  data_grid(x1,x2) %>%
  gather_predictions(mod1, mod2)
grid

#Visualize the results for both models on one plot by using faceting:
ggplot(sim3, aes(x1, y, color=x2)) +
  geom_point() +
  geom_line(data=grid, aes(y=pred)) + 
  facet_wrap(~model)

#We can see the model which uses + has different intercepts, but the same slope for each line.
#The model which uses * has differing slopes and intercepts.  
#Which is better?
sim3 <- sim3 %>%
  gather_residuals(mod1, mod2)

ggplot(sim3, aes(x1, resid, color=x2)) +
  geom_point() +
  facet_grid(model ~ x2)

#There's little obvious pattern for the residuals from model2




#23.4.3: ***************************** Interactions (2 continuous) *****************************
mod1 <- lm(y ~ x1 + x2, data=sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

grid <- sim4 %>% 
  data_grid(
    x1 = seq_range(x1, 5),
    x2 = seq_range(x2, 5)
  ) %>% 
  gather_predictions(mod1, mod2)

grid

#Note use of seq_range() in data_grid().  Instead of using every unique value of x, this uses
#a regularly spaced grid of of 5 values between min and max numbers.  A useful technique!

#Two other useful arguments to seq_range():
  #pretty=TRUE will generate a "pretty" sequence (looks nice to the eye).  This is useful when 
  #producing tables of output:
seq_range(c(0.0123, 0.923423),n=5)
seq_range(c(0.0123, 0.923423),n=5, pretty=TRUE)

  #trim = 0.1 will trim off 10% of the tail values.  This is great when variables have long-tailed distribution:
x1 <- rcauchy(10)
seq_range(x1, n=5)
seq_range(x1, n=5, trim=0.1)
seq_range(x1, n=5, trim=0.25)
seq_range(x1, n=5, trim=0.5)

  #expand = 0.1 is in some sense the opposite - it exapnds the range by 10%:
x2 <- c(0,1)
seq_range(x2, n=5)
seq_range(x2, n=5, expand=0.10)
seq_range(x2, n=5, expand=0.25)
seq_range(x2, n=5, expand=0.50)

#Try to visualize that model.  Since we have 2 continuous predictors, we can imagine a 3D surface:
ggplot(grid, aes(x1, x2)) +
  geom_tile(aes(fill = pred)) +
  facet_wrap(~ model)


#23.4.4: ***************************** Transformations *****************************


#23.5: ***************************** Missing Values *****************************
#R???s default behaviour is to silently drop them, but 
#options(na.action = na.warn) (run in the prerequisites), makes sure you get a warning.
df <- tribble(
  ~x, ~y,
  1, 2.2,
  2, NA, 
  3, 3.5,
  4, 8.3,
  NA, 10
)

mod <- lm(y ~ x, data = df)

#To suppress the warning:
mod <- lm(y ~ x, data=df, na.action = na.exclude)

#to see how many observations were used:
nobs(mod)


#23.6: ***************************** Other Model Families *****************************
#Some other model families include:
  #Generalized Linear Models - stats::glm() - 
  #Generalized Additive Models - mgv::gam() - 
  #Penalized Linear Models - glmnet::glmnet() - 
  #Robust Linear Models - MASS:rlm() - 
  #Trees - rpart::rpart() - 
    #random forests - randomForest::randomForest() - 
    #gradient boosting machines - xgboost::xgboost() - 







#24: ******************************* Model Building *****************************
#24.1: ***************************** Introduction *****************************



#24.2: ***************************** Why are low-quality Diamonds More Expensive?*****************************



#24.3: ***************************** What Affects the Number of Daily Flights?  *****************************



#24.4: ***************************** Learning More about Models *****************************
#Statistical Modeling: A Fresh Approach by Danny Kaplan, http://www.mosaic-web.org/go/StatisticalModeling/. 
#This book provides a gentle introduction to modelling, where you build your intuition, mathematical tools, 
#and R skills in parallel. The book replaces a traditional ???introduction to statistics??? course, 
#providing a curriculum that is up-to-date and relevant to data science.

#An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani, http://www-bcf.usc.edu/~gareth/ISL/.
#This book presents a family of modern modelling techniques collectively known as 
#statistical learning. For an even deeper understanding of the math behind the models, read the classic 
#Elements of Statistical Learning by Hastie, Tibshirani, and Friedman, http://statweb.stanford.edu/~tibs/ElemStatLearn/.

#Applied Predictive Modeling by Max Kuhn and Kjell Johnson, http://appliedpredictivemodeling.com. 
#This book is a companion to the caret package and provides practical tools for dealing with real-life 
#predictive modelling challenges.




#25: ******************************* Many Models *****************************
#25.1: ***************************** Introduction *****************************



#25.2: ***************************** Gapminder *****************************



#25.3: ***************************** List-columns *****************************



#25.4: ***************************** Creating List-columns *****************************



#25.5: ***************************** Simplifying List-columns *****************************



#25.6: ***************************** Making Tidy Data with Broom *****************************